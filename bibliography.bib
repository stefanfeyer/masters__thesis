@article{ReitererHumancomputerInteractionGroupUniversity11,
	title = {Human-Computer Interaction group {{University}} of {{Konstanz}}, {{Germany}}},
	author = {Reiterer, Harald},
	date = {2011-11-01},
	journaltitle = {interactions},
	shortjournal = {interactions},
	volume = {18},
	pages = {82},
	issn = {10725520},
	doi = {10.1145/2029976.2029997},
	url = {http://dl.acm.org/citation.cfm?doid=2029976.2029997},
	urldate = {2020-08-12},
	langid = {english},
	number = {6}
}
@inproceedings{muckell,
	author = {Muckell, Jonathan and Young, Yuchi and Leventhal, Mitch},
	title = {A Wearable Motion Tracking System to Reduce Direct Care Worker Injuries: An Exploratory Study},
	year = {2017},
	isbn = {9781450352499},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3079452.3079493},
	doi = {10.1145/3079452.3079493},
	abstract = {Patients with functional disabilities often require assistance to perform basic everyday activities, such as bathing, dressing, and getting into/out of bed. These activities typically require the direct care worker (DCW) to transfer (lift and move) the patient from one location to another. These patient transfers are a common cause of injury to health care workers. In fact, depending on the job site, on average a staggering 4% of DCWs are injured every year. Following proper lifting and transfer procedures can dramatically reduce the risk of injury. This research demonstrates that data collected from motion tracking systems, combined with computational analysis can detect risky patient transfer behavior. Testing of the system occurred as part of an exploratory study in an assisted living facility. Two common types of transfers were tested: transfers from bed to shower chair, and transfers from shower chair to wheelchair. These scenarios were tested on two types of patients, one that was completely disabled, and one that was partially disabled. Two major results were determined from this study: (1) risky patient transfer behavior is common in the assisted living facility, and (2) this behavior can be adequately detected via wearable motion tracking sensors. The longer term research goal is to extend these preliminary results to construct a fully wearable motion tracking system that can be used as a tool to reinforce proper lifting and transfer protocols to reduce work-related injuries among DCWs.},
	booktitle = {Proceedings of the 2017 International Conference on Digital Health},
	pages = {202–206},
	numpages = {5},
	keywords = {motion tracking, wearable computing, patient transfers, injury prevention},
	location = {London, United Kingdom},
	series = {DH '17}
}
@inproceedings{YouMove,
	author = {Anderson, Fraser and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
	title = {YouMove: Enhancing Movement Training with an Augmented Reality Mirror},
	year = {2013},
	isbn = {9781450322683},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2501988.2502045},
	doi = {10.1145/2501988.2502045},
	abstract = {YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.},
	booktitle = {Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},
	pages = {311–320},
	numpages = {10},
	keywords = {training, augmented reality, movement, 3d, guidance., motor learning, full body, learning},
	location = {St. Andrews, Scotland, United Kingdom},
	series = {UIST '13}
}
@ARTICLE{vrdancetrainer,
	author={J. C. P. {Chan} and H. {Leung} and J. K. T. {Tang} and T. {Komura}},
	journal={IEEE Transactions on Learning Technologies}, 
	title={A Virtual Reality Dance Training System Using Motion Capture Technology}, 
	year={2011},
	volume={4},
	number={2},
	pages={187-195},
	doi={10.1109/TLT.2010.27}
}
@inproceedings{outsideme,
	author = {Yan, Shuo and Ding, Gangyi and Guan, Zheng and Sun, Ningxiao and Li, Hongsong and Zhang, Longfei},
	title = {OutsideMe: Augmenting Dancer's External Self-Image by Using A Mixed Reality System},
	year = {2015},
	isbn = {9781450331463},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2702613.2732759},
	doi = {10.1145/2702613.2732759},
	abstract = {External self-image is often used as an effective tool to enhance dancing technique, choreography, creativity, and expression. The traditional tools of presenting external image, such as mirrors or videos, are limited in their mobility, perspective, and immediacy. To address the issue, we present OutsideMe, a vision-sync mixed reality system that enables dancers see their body movements as external observers through a head-mounted display (HMD) device. This system captures dancer's posture and blends it into scenes from the dancer's original field of view in an interactive frame rate. The dancers can observe themselves without distracting their presence identities. In this research, we develop four work modes for supporting dancer's training, and carry out a feasibility study and a user study. The feedbacks from the participants performing various dancing styles are analyzed and discussed. The preliminary experimental results support our design.},
	booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
	pages = {965–970},
	numpages = {6},
	keywords = {augmented human, external self-image, choreography, mixed reality, visual blending},
	location = {Seoul, Republic of Korea},
	series = {CHI EA '15}
}
@inproceedings{performancetraining,
	author = {Chan, Jacky and Leung, Howard and Tang, Kai Tai and Komura, Taku},
	title = {Immersive Performance Training Tools Using Motion Capture Technology},
	year = {2007},
	isbn = {9789639799066},
	publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
	address = {Brussels, BEL},
	abstract = {Traditionally, students can undergo performance training by imitating teachers' motions and improving their moves by following teachers' advice. However, teachers are not always available and there is a lack of effective self-training tools. In this paper, we propose an immersive performance training system with motion capture technology. Our aim is to provide a virtual training environment for performance training. In the environment, the student's performance is analyzed by our system and a virtual teacher will give feedback for his/her improvement. As a result, the student can self-train and get feedback the same way as in a traditional performance training lesson. We will present our prototype implementation of a dance education system according to our design of immersive performance training system with motion capture technology.},
	booktitle = {Proceedings of the First International Conference on Immersive Telecommunications},
	articleno = {7},
	numpages = {6},
	keywords = {human computer interaction, immersive VR application, performance training, 3d human motion analysis},
	location = {Bussolengo, Verona, Italy},
	series = {ImmersCom '07}
}
@INPROCEEDINGS{mrdancetrainer,
	author={K. {Hachimura} and H. {Kato} and H. {Tamura}},
	booktitle={RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)}, 
	title={A prototype dance training support system with motion capture and mixed reality technologies}, 
	year={2004},
	volume={},
	number={},
	pages={217-222},
	doi={10.1109/ROMAN.2004.1374759}
}
@inproceedings{freethrowsimulator,
	author = {Covaci, Alexandra and Olivier, Anne-H\'{e}l\`{e}ne and Multon, Franck},
	title = {Third Person View and Guidance for More Natural Motor Behaviour in Immersive Basketball Playing},
	year = {2014},
	isbn = {9781450332538},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2671015.2671023},
	doi = {10.1145/2671015.2671023},
	abstract = {The use of Virtual Reality (VR) in sports training is now widely studied with the perspective to transfer motor skills learned in virtual environments (VEs) to real practice. However precision motor tasks that require high accuracy have been rarely studied in the context of VE, especially in Large Screen Image Display (LSID) platforms. An example of such a motor task is the basketball free throw, where the player has to throw a ball in a 46cm wide basket placed at 4.2m away from her. In order to determine the best VE training conditions for this type of skill, we proposed and compared three training paradigms. These training conditions were used to compare the combinations of different user perspectives: first (1PP) and third-person (3PP) perspectives, and the effectiveness of visual guidance. We analysed the performance of eleven amateur subjects who performed series of free throws in a real and immersive 1:1 scale environment under the proposed conditions. The results show that ball speed at the moment of the release in 1PP was significantly lower compared to real world, supporting the hypothesis that distance is underestimated in large screen VEs. However ball speed in 3PP condition was more similar to the real condition, especially if combined with guidance feedback. Moreover, when guidance information was proposed, the subjects released the ball at higher - and closer to optimal - position (5-7% higher compared to no-guidance conditions). This type of information contributes to better understand the impact of visual feedback on the motor performance of users who wish to train motor skills using immersive environments. Moreover, this information can be used by exergames designers who wish to develop coaching systems to transfer motor skills learned in VEs to real practice.},
	booktitle = {Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology},
	pages = {55–64},
	numpages = {10},
	keywords = {visual feedback, performance, basketball training, perception of distance in VR, immersive room},
	location = {Edinburgh, Scotland},
	series = {VRST '14}
}
@InProceedings{trainingphysicalskills,
	author="Kojima, Taihei
	and Hiyama, Atsushi
	and Miura, Takahiro
	and Hirose, Michitaka",
	editor="Yamamoto, Sakae",
	title="Training Archived Physical Skill through Immersive Virtual Environment",
	booktitle="Human Interface and the Management of Information. Information and Knowledge in Applications and Services",
	year="2014",
	publisher="Springer International Publishing",
	address="Cham",
	pages="51--58",
	abstract="The basic of training physical skills is to imitate instructor's motion. Observation is the very first step to copy the motion of instructor when at the beginning of learning sports of artisanship. However, beginners face difficulties in imitating at the start since they do not have somesthetic image of the movement. In order to help learning physical skills, we propose Immersive virtual environment using head mounted display that indicates 3D motion of instructor super imposed on learner's body. By using this system, learners try to match its own form to instructor's 3D model to imitate instructor's motion from first person view in virtual environment. At the early stage of this research, we tried to transfer pitching skill in baseball. We evaluated the effectiveness of proposed system by measuring throwing distance.",
	isbn="978-3-319-07863-2"
}
@inbook{motionma,
	author = {Velloso, Eduardo and Bulling, Andreas and Gellersen, Hans},
	title = {MotionMA: Motion Modelling and Analysis by Demonstration},
	year = {2013},
	isbn = {9781450318990},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2470654.2466171},
	abstract = {Particularly in sports or physical rehabilitation, users have to perform body movements in a specific manner for the exercises to be most effective. It remains a challenge for experts to specify how to perform such movements so that an automated system can analyse further performances of it. In a user study with 10 participants we show that experts' explicit estimates do not correspond to their performances. To address this issue we present MotionMA, a system that: (1) automatically extracts a model of movements demonstrated by one user, e.g. a trainer, (2) assesses the performance of other users repeating this movement in real time, and (3) provides real-time feedback on how to improve their performance. We evaluated the system in a second study in which 10 other participants used the system to demonstrate arbitrary movements. Our results demonstrate that MotionMA is able to extract an accurate movement model to spot mistakes and variations in movement execution.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {1309–1318},
	numpages = {10}
}
@inbook{physioathome,
	author = {Tang, Richard and Yang, Xing-Dong and Bateman, Scott and Jorge, Joaquim and Tang, Anthony},
	title = {Physio@Home: Exploring Visual Guidance and Feedback Techniques for Physiotherapy Exercises},
	year = {2015},
	isbn = {9781450331456},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2702123.2702401},
	abstract = {Physiotherapy patients exercising at home alone are at risk of re-injury since they do not have corrective guidance from a therapist. To explore solutions to this problem, we designed Physio@Home, a prototype that guides people through prerecorded physiotherapy exercises using real-time visual guides and multi-camera views. Our design addresses several aspects of corrective guidance, including: plane and range of movement, joint positions and angles, and extent of movement. We evaluated our design, com-paring how closely people could follow exercise movements under various feedback conditions. Participants were most accurate when using our visual guide and multi-views. We provide suggestions for exercise guidance systems drawn from qualitative findings on visual feedback complexity.},
	booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
	pages = {4123–4132},
	numpages = {10}
}
@article{kinohaptics,
	author = {Rajanna, Vijay and Vo, Patrick and Barth, Jerry and Mjelde, Matthew and Grey, Trevor and Oduola, Cassandra and Hammond, Tracy},
	year = {2015},
	month = {12},
	pages = {},
	title = {KinoHaptics: An Automated, Wearable, Haptic Assisted, Physio-therapeutic System for Post-surgery Rehabilitation and Self-care},
	volume = {40},
	journal = {Journal of Medical Systems},
	doi = {10.1007/s10916-015-0391-3}
}
@inproceedings{sleevear,
	author = {Sousa, Maur\'{\i}cio and Vieira, Jo\~{a}o and Medeiros, Daniel and Arsenio, Artur and Jorge, Joaquim},
	title = {SleeveAR: Augmented Reality for Rehabilitation Using Realtime Feedback},
	year = {2016},
	isbn = {9781450341370},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2856767.2856773},
	doi = {10.1145/2856767.2856773},
	abstract = {We present an intelligent user interface that allows people to perform rehabilitation exercises by themselves under the offline supervision of a therapist. Every year, many people suffer injuries that require rehabilitation. This entails considerable time overheads since it requires people to perform specified exercises under the direct supervision of a therapist. Therefore it is desirable that patients continue performing exercises outside the clinic (for instance at home, thus without direct supervision), to complement in-clinic physical therapy. However, to perform rehabilitation tasks accurately, patients need appropriate feedback, as otherwise provided by a physical therapist, to ensure that these unsupervised exercises are correctly executed. Different approaches address this problem, providing feedback mechanisms to aid rehabilitation. Unfortunately, test subjects frequently report having trouble to completely understand the feedback thus provided, which makes it hard to correctly execute the prescribed movements. Worse, injuries may occur due to incorrect performance of the prescribed exercises, which severely hinders recovery. SleeveAR is a novel approach to provide real-time, active feedback, using multiple projection surfaces to provide effective visualizations. Empirical evaluation shows the effectiveness of our approach as compared to traditional video-based feedback. Our experimental results show that our intelligent UI can successfully guide subjects through an exercise prescribed (and demonstrated) by a physical therapist, with performance improvements between consecutive executions, a desirable goal to successful rehabilitation.},
	booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
	pages = {175–185},
	numpages = {11},
	keywords = {augmented reality, rehabilitation, projection-based systems},
	location = {Sonoma, California, USA},
	series = {IUI '16}
}
@inproceedings{ararm,
	author = {Han, Ping-Hsuan and Chen, Kuan-Wen and Hsieh, Chen-Hsin and Huang, Yu-Jie and Hung, Yi-Ping},
	title = {AR-Arm: Augmented Visualization for Guiding Arm Movement in the First-Person Perspective},
	year = {2016},
	isbn = {9781450336802},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2875194.2875237},
	doi = {10.1145/2875194.2875237},
	abstract = {In many activities, such as martial arts, physical exercise, and physiotherapy, the users are asked to perform a sequence of body movements with highly accurate arm positions. Sometimes, the movements are too complicated for users to learn, even by imitating the action of the coach directly. This paper presents a fully immersive augmented reality (AR) system, which provides egocentric hints to guide the arm movement of the user via a video see-through head-mounted display (HMD). By using this system, the user can perform the exactitude of arm movement simply by moving his arms to follow and match the virtual arms, rendered from coach's movement of database, in the first-person view. To ensure the rendered virtual arms correctly aligned with the user's real shoulders, a calibration method is proposed to estimate the length of the user's arms and the positions of his head and shoulders in advance. In addition, we apply the system to Tai-Chi-Chuan practicing, our preliminary study has shown that the proposed egocentric hints can provide intuitive guidance for users to follow the arm movement of the coach with exactitude.},
	booktitle = {Proceedings of the 7th Augmented Human International Conference 2016},
	articleno = {31},
	numpages = {4},
	keywords = {Augmented Reality, Wearable Interaction, Body Movement Guidance, Visualization},
	location = {Geneva, Switzerland},
	series = {AH '16}
}
@ARTICLE{justfollowme,
	author={U. {Yang} and G. J. {Kim}},
	journal={Presence}, 
	title={Implementation and Evaluation of “Just Follow Me”: An Immersive, VR-Based, Motion-Training System}, 
	year={2002},
	volume={11},
	number={3},
	pages={304-323},
	doi={10.1162/105474602317473240}
}
@inproceedings{stylo,
	author = {Katzakis, Nicholas and Tong, Jonathan and Ariza, Oscar and Chen, Lihan and Klinker, Gudrun and R\"{o}der, Brigitte and Steinicke, Frank},
	title = {Stylo and Handifact: Modulating Haptic Perception through Visualizations for Posture Training in Augmented Reality},
	year = {2017},
	isbn = {9781450354868},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3131277.3132181},
	doi = {10.1145/3131277.3132181},
	abstract = {Stylo-Handifact is a novel spatial user interface consisting of a haptic device (i. e., Stylo) attached to the forearm and a visualization of a virtual hand (i. e., Handifact), which in combination provide visuo-haptic feedback for posture training applications.In this paper we evaluate the mutual effects of Handifact and Stylo on visuo-haptic sensations in a psychophysical experiment. The results show that a visual stimulus can modulate the perceived strength of a haptic stimulus by more than 5%. A wrist docking task indicates that Stylo-Handifact results in improved task completion time as compared to a state-of-the-art technique.},
	booktitle = {Proceedings of the 5th Symposium on Spatial User Interaction},
	pages = {58–67},
	numpages = {10},
	keywords = {wearable, posture training, perception, integration, visual, multisensory, sensory, haptic, interface},
	location = {Brighton, United Kingdom},
	series = {SUI '17}
}
@inproceedings{elearningma,
	author = {Komura, Taku and Lam, Beta and Lau, Rynson W. H. and Leung, Howard},
	title = {E-Learning Martial Arts},
	year = {2006},
	isbn = {3540490272},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/11925293_22},
	doi = {10.1007/11925293_22},
	abstract = {Traditionally, when people want to learn martial arts, they have to go to training clubs and learn under the coach together with the other students. To reduce the tuition fees, there are usually a lot of students under a single coach and hence, it is difficult for the students to get enough suggestions in the class. It would be far easier if the students could practice themselves at home and ask for suggestions from a virtual coach in the computer. Occasionally, in case they find difficulties going to the next step, they may then approach the real coach for suggestions and training. In this paper, we propose such a training system based on the motion capture system. The system automatically analyzes the motions of the player and gives suggestions. The students can also view the martial art techniques stored in the system or their own techniques captured by the motion capture system from various points of view in order to gain objective ideas of the techniques.},
	booktitle = {Proceedings of the 5th International Conference on Advances in Web Based Learning},
	pages = {239–248},
	numpages = {10},
	keywords = {web-based learning, automatic motion analysis, martial arts, motion capture},
	location = {Penang, Malaysia},
	series = {ICWL'06}
}
@inproceedings{mythaichicoaches,
	author = {Han, Ping-Hsuan and Chen, Yang-Sheng and Zhong, Yilun and Wang, Han-Lei and Hung, Yi-Ping},
	title = {My Tai-Chi Coaches: An Augmented-Learning Tool for Practicing Tai-Chi Chuan},
	year = {2017},
	isbn = {9781450348355},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3041164.3041194},
	doi = {10.1145/3041164.3041194},
	abstract = {Tai-Chi Chuan (TCC) is a famous physical exercise and well-known for being able to effectively promote physical well-being. Many people have been interested in learning TCC at the beginning, but eventually failed in mastering it due to the lack of a constantly accompanying master on the side. In this paper, we present an augmented-learning tool, called "My Tai-Chi Coaches", for learning TCC. By wearing an optical see-through Head-Mounted Display (HMD), the users can have their own private coaches-on-demand that will guide them in practicing TCC. To solve the "attention-sticking" problem, we propose the use of "redundant coaches" and high-lighting the primary coach at every instant. When the user wants to adjust his posture to mimic the coach's movement, he can simply suspend his motion, and then the drone will fly to a proper position to capture the images of the user's posture, and display them on an augmented mirror placed near by the highlighted or gazed coach. In addition to learning TCC, the proposed augmented-learning tool can also be used for learning dancing, yoga, sporting, and for rehabilitation.},
	booktitle = {Proceedings of the 8th Augmented Human International Conference},
	articleno = {25},
	numpages = {4},
	keywords = {mixed reality, physical activity learning, Tai-Chi Chuan, drone, augmented mirror},
	location = {Silicon Valley, California, USA},
	series = {AH '17}
}
@InProceedings{rtgesturerecognistion,
	author="Portillo-Rodriguez, Otniel
	and Sandoval-Gonzalez, Oscar O.
	and Ruffaldi, Emanuele
	and Leonardi, Rosario
	and Avizzano, Carlo Alberto
	and Bergamasco, Massimo",
	editor="Pirhonen, Antti
	and Brewster, Stephen",
	title="Real-Time Gesture Recognition, Evaluation and Feed-Forward Correction of a Multimodal Tai-Chi Platform",
	booktitle="Haptic and Audio Interaction Design",
	year="2008",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="30--39",
	abstract="This paper presents a multimodal system capable to understand and correct in real-time the movements of Tai-Chi students through the integration of audio-visual-tactile technologies. This platform acts like a virtual teacher that transfers the knowledge of five Tai-Chi movements using feed-back stimuli to compensate the errors committed by a user during the performance of the gesture. The fundamental components of this multimodal interface are the gesture recognition system (using k-means clustering, Probabilistic Neural Networks (PNN) and Finite State Machines (FSM)) and the real-time descriptor of motion which is used to compute and qualify the actual movements performed by the student respect to the movements performed by the master, obtaining several feedbacks and compensating this movement in real-time varying audio-visualtactile parameters of different devices. The experiments of this multimodal platform have confirmed that the quality of the movements performed by the students is improved significantly.",
	isbn="978-3-540-87883-4"
}
@inproceedings{onebody,
	author = {Hoang, Thuong N. and Reinoso, Martin and Vetere, Frank and Tanin, Egemen},
	title = {Onebody: Remote Posture Guidance System Using First Person View in Virtual Environment},
	year = {2016},
	isbn = {9781450347631},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2971485.2971521},
	doi = {10.1145/2971485.2971521},
	abstract = {We present Onebody, a virtual reality system for remote posture guidance during sports or physical activity training, such as martial arts, yoga or dance, using first person perspective. The system uses skeletal tracking of the instructor and the students, rendered as virtual avatars. Using a virtual reality headset, the student can visualise the movement of the instructor's avatar, rendered in place of their own body. Onebody provides a first person perspective of the movement instruction, allowing the student to step into the instructor's body. We conducted a study to compare the performance of Onebody in terms of posture matching accuracy and user's preference, with existing techniques of delivering movement instructions, including pre-recorded video, video conferencing and third person view virtual reality. The result indicated that Onebody offers better posture accuracy in delivering movement instructions.},
	booktitle = {Proceedings of the 9th Nordic Conference on Human-Computer Interaction},
	articleno = {25},
	numpages = {10},
	keywords = {Virtual reality, posture guidance, first person view},
	location = {Gothenburg, Sweden},
	series = {NordiCHI '16}
}
@INPROCEEDINGS{thaichichua,
	author={ {Philo Tan Chua} and R. {Crivella} and B. {Daly} and  {Ning Hu} and R. {Schaaf} and D. {Ventura} and T. {Camill} and J. {Hodgins} and R. {Pausch}},
	booktitle={IEEE Virtual Reality, 2003. Proceedings.}, 
	title={Training for physical tasks in virtual environments: Tai Chi}, 
	year={2003},
	volume={},
	number={},
	pages={87-94},
	doi={10.1109/VR.2003.1191125}
}
@ARTICLE{tikl,
	author={J. {Lieberman} and C. {Breazeal}},
	journal={IEEE Transactions on Robotics}, 
	title={TIKL: Development of a Wearable Vibrotactile Feedback Suit for Improved Human Motor Learning}, 
	year={2007},
	volume={23},
	number={5},
	pages={919-926},
	doi={10.1109/TRO.2007.907481}
}
@inproceedings{lightguide,
	author = {Sodhi, Rajinder and Benko, Hrvoje and Wilson, Andrew},
	title = {LightGuide: Projected Visualizations for Hand Movement Guidance},
	year = {2012},
	isbn = {9781450310154},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2207676.2207702},
	doi = {10.1145/2207676.2207702},
	abstract = {LightGuide is a system that explores a new approach to gesture guidance where we project guidance hints directly on a user's body. These projected hints guide the user in completing the desired motion with their body part which is particularly useful for performing movements that require accuracy and proper technique, such as during exercise or physical therapy. Our proof-of-concept implementation consists of a single low-cost depth camera and projector and we present four novel interaction techniques that are focused on guiding a user's hand in mid-air. Our visualizations are designed to incorporate both feedback and feedforward cues to help guide users through a range of movements. We quantify the performance of LightGuide in a user study comparing each of our on-body visualizations to hand animation videos on a computer display in both time and accuracy. Exceeding our expectations, participants performed movements with an average error of 21.6mm, nearly 85% more accurately than when guided by video.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {179–188},
	numpages = {10},
	keywords = {on-demand interfaces, on-body computing, tracking, spatial augmented reality, appropriated surfaces},
	location = {Austin, Texas, USA},
	series = {CHI '12}
}
@inproceedings{nursecare,
	author = {D\"{u}rr, Maximilian and Gr\"{o}schel, Carla and Pfeil, Ulrike and Reiterer, Harald},
	title = {NurseCare: Design and 'In-The-Wild' Evaluation of a Mobile System to Promote the Ergonomic Transfer of Patients},
	year = {2020},
	isbn = {9781450367080},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3313831.3376851},
	doi = {10.1145/3313831.3376851},
	abstract = {Nurses are frequently required to transfer patients as part of their daily duties. However, the manual transfer of patients is a major risk factor for injuries to the back. Although the Kinaesthetics Care Conception can help to address this issue, existing support for the integration of the concept into nursing-care practice is low. We present NurseCare, a mobile system that aims to promote the practical application of ergonomic patient transfers based on the Kinaesthetics Care Conception. NurseCare consists of a wearable and a smartphone app. Key features of NurseCare include mobile accessible instructions for ergonomic patient transfers, in-situ feedback for the risky bending of the back, and long-term feedback. We evaluated NurseCare in a nine participant 'in-the-wild' evaluation. Results indicate that NurseCare can facilitate ergonomic work while providing a high user experience adequate to the nurses' work domain, and reveal how NurseCare can be incorporated in given practices.},
	booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages = {1–13},
	numpages = {13},
	keywords = {nursing care, mobile system, `in-the-wild' evaluation},
	location = {Honolulu, HI, USA},
	series = {CHI '20}
}
@inproceedings{kitt,
	title={KiTT - The Kinaesthetics Transfer Teacher : Design and Evaluation of a Tablet-based System to Promote the Learning of Ergonomic Patient Transfers},
	year={2021},
	doi={10.1145/3411764.3445496},
	isbn={978-1-4503-8096-6},
	address={New York},
	publisher={ACM},
	booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI 2021)},
	author={Dürr, Maximilian and Borowski, Marcel and Gröschel, Carla and Pfeil, Ulrike and Müller, Jens and Reiterer, Harald}
}
@book{mlbook,
	title={Motor Control and Learning: A Behavioral Emphasis},
	author={Schmidt, R.A. and Lee, T.D.},
	isbn={9780736042581},
	lccn={2004020514},
	year={2005},
	publisher={Human Kinetics}
}
@inproceedings{centricitycontinuum,
	author = {Wang, Wenbi},
	title = {Dynamic Viewpoint Tethering: Controlling a Virtual Camera for Effective Navigation in Virtual Environments},
	year = {2001},
	isbn = {1581133405},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/634067.634124},
	doi = {10.1145/634067.634124},
	abstract = {Dynamic viewpoint tethering is an innovative display technique, which has been proposed to support effective navigation in large-scale virtual environments by integrating information from different frames of reference. The dynamic tether incorporates principles which are known from the older technique of frequency separation, and in many ways resembles a mass-spring-damper system. This study examines the effect of dynamic viewpoint tethering on human users' performance on both local guidance and global awareness tasks. The research results support the design of display systems in improving human-computer interaction in teleoperation tasks.},
	booktitle = {CHI '01 Extended Abstracts on Human Factors in Computing Systems},
	pages = {93–94},
	numpages = {2},
	keywords = {global awareness, dynamically tethered displays, navigation, local guidance, virtual environments, virtual cameras},
	location = {Seattle, Washington},
	series = {CHI EA '01}
}
@inproceedings{samesetup,
	author = {Sra, Misha and Mottelson, Aske and Maes, Pattie},
	title = {Your Place and Mine: Designing a Shared VR Experience for Remotely Located Users},
	year = {2018},
	isbn = {9781450351980},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3196709.3196788},
	doi = {10.1145/3196709.3196788},
	abstract = {Virtual reality can help realize mediated social experiences where distance disappears and we interact as richly with those around the world as we do with those in the same room. The design of social virtual experiences presents a challenge for remotely located users with room-scale setups like those afforded by recent commodity virtual reality devices. Since users inhabit different physical spaces that may not be the same size, a mapping to a shared virtual space is needed for creating experiences that allow everyone to use real walking for locomotion. We designed three mapping techniques that enable users from diverse room-scale setups to interact together in virtual reality. Results from our user study (N = 26) show that our mapping techniques positively influence the perceived degree of togetherness and copresence while the size of each user's tracked space influences individual presence.},
	booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
	pages = {85–97},
	numpages = {13},
	keywords = {virtual reality, social, embodiment, dancing, room-scale vr, remote collaboration},
	location = {Hong Kong, China},
	series = {DIS '18}
}
@article{mmh,
	title = {Manual Material Handling: A Classification Scheme},
	journal = {Procedia Technology},
	volume = {24},
	pages = {568-575},
	year = {2016},
	note = {International Conference on Emerging Trends in Engineering, Science and Technology (ICETEST - 2015)},
	issn = {2212-0173},
	doi = {https://doi.org/10.1016/j.protcy.2016.05.114},
	url = {https://www.sciencedirect.com/science/article/pii/S2212017316302031},
	author = {R. Rajesh},
	abstract = {Ergonomic evaluation of Manual Material Handling (MMH) has largely been based on task analysis approach where the job are broken down into simpler tasks and studied. But there is lack of clarity in the use of terms defining various MMH activities. The challenge in classifying MMH arises because of the dependence of man-machine interaction on multiple worksystem characteristics. This paper presents a classification scheme for MMH tasks. Towards making a classification scheme the work system characteristics are examined and the important dimensions from those are identified that are able to differentiate the nature of MMH exposure. Suitable examples for each class are presented. The methods for collecting biomechanical and physiological responses, and nature of ergonomic analysis required are discussed. A qualitative judgment on exposure magnitude and measurement cost is made. Finally, critical issues and scope for research is presented.}
}
@article{veimprovesml,
	Title = {Virtual Environment Training Improves Motor Performance in Two Patients with Stroke: Case Report},
	Author = {Holden, Maureen and Todorov, Emanuel and Callahan, Janet and Bizzi, Emilio;},
	Journal = {Journal of Neurologic Physical Therapy},
	year = {1999},
	volume = {23},
	doi = {},
	url = {https://journals.lww.com/jnpt/Fulltext/1999/23020/Virtual_Environment_Training_Improves_Motor.13.aspx},
}
@MISC{mrcontinuum,
	author = {Paul Milgram and Fumio Kishino},
	title = {A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS},
	year = {1994}
}
@inproceedings{max,
	author = {D\"{u}rr, Maximilian and Weber, Rebecca and Pfeil, Ulrike and Reiterer, Harald},
	title = {EGuide: Investigating Different Visual Appearances and Guidance Techniques for Egocentric Guidance Visualizations},
	year = {2020},
	isbn = {9781450361071},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3374920.3374945},
	doi = {10.1145/3374920.3374945},
	abstract = {Mid-air arm movements are important for various activities. However, common resources for their self-directed practice require practitioners to divide their focus between an external source (e.g., a video screen) and moving. Past research found benefits for egocentric guidance visualizations compared to common resources. However, there is limited evidence about how such visualizations should look and behave. EGuide supports the investigation of different egocentric visualizations for the guidance of mid-air arm movements. We compared two visual appearances for egocentric guidance visualizations that differ in their shape (look), and three guidance techniques that differ by how they guide a user (behavior). For visualizations with a continuously moving guidance technique, our results suggest a higher movement accuracy for a realistic than an abstract shape. For user experience and preference, our results suggest that visualizations with an abstract shape and a guidance technique that visualizes important postures should not pause at important postures.},
	booktitle = {Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction},
	pages = {311–322},
	numpages = {12},
	keywords = {virtual reality, egocentric guidance, learning, arm movements},
	location = {Sydney NSW, Australia},
	series = {TEI '20}
}
@INPROCEEDINGS{perspectivematters,
	author={X. {Yu} and K. {Angerbauer} and P. {Mohr} and D. {Kalkofen} and M. {Sedlmair}},
	booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
	title={Perspective Matters: Design Implications for Motion Guidance in Mixed Reality}, 
	year={2020},
	volume={},
	number={},
	pages={577-587},
	doi={10.1109/ISMAR50242.2020.00085}
}
@article{hornbaek,
	author = {Hornb\ae{}k, Kasper},
	title = {Some Whys and Hows of Experiments in Human–Computer Interaction},
	year = {2013},
	issue_date = {June 2013},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
	volume = {5},
	number = {4},
	issn = {1551-3955},
	url = {https://doi.org/10.1561/1100000043},
	doi = {10.1561/1100000043},
	abstract = {Experiments help to understand human–computer interaction and to characterize the value of user interfaces. Yet, few intermediate guidelines exist on how to design, run, and report experiments. The present monograph presents such guidelines. We briefly argue why experiments are invaluable for advancing human–computer interaction beyond technical innovation.We then identify heuristics of doing good experiments, including how to build on existing work in devising hypotheses and selecting measures; how to craft challenging comparisons, rather than biased win–lose setups; how to design experiments so as to rule out alternative explanations; how to provide evidence for conclusions; and how to narrate findings. These heuristics are exemplified by excellent experiments in human–computer interaction.},
	journal = {Found. Trends Hum.-Comput. Interact.},
	month = jun,
	pages = {299–373},
	numpages = {75}
}
